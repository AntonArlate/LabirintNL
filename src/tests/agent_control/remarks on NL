 # <<< Описание переменных и формул >>>
    # x - Входящее значение
    # t - Линейный результат; Выход нейрона без приминения активации
    # w - вес связи между нейроном
    # b - bias, сдвиг; 1 * на некий W
    # F() - Функция активации
    # h - F(t); нелинейное значение полученое после функции активации F
    # >> далее возможно свободная интерпретация
    # dE - вектор ошибок
    # dE|dh - читать как вектор ошибок по вектору h
    # dh|dt - это производная функция
    # dE|dt = dE|dh * dh|dt; вектор ошибок по вектору t (от функции активации); 
    # т.е. получая на вход E нейрон должен умножить её на производную и уже в таком виде записать на выход; 
    # не забывать что ошибку считаем обратно (вход в конце сети)
    # dE|dw - вектор ошибок на связях.
    # dE|dw = dE|dt * dt|dw; тое чтобы получит ошибку связи нужно ошибку на выходе нейрона умножить на dt|dw
    # dt|dw - упрощённо говоря это выход предыдущего слоя.
    # dE|dw = xT * dE|dt; читать как транспонированный вход (это вектор) умножить на вектор выхода ошибки нейрона. Умножение матричное.
    # dE|db - ошибка по весам смещения будет равна ошибке нейрона.
    # dE|dx = dE|dt * dt|dx
    # dE|dx = dE|dt * w + [...] ; тут из-за форматирования не понятно может быть
    #     Если в предыдущих функциях для каждого результата был лишь один набор индексов. 
    #     То здесь для одного индекса x нам надо сложить результат сложения всех соответствующих этому индексу связей со всеми нейронами
    #     Так как w это матрица nm размера. То n=индекс(x) . m=индекс(t)
    #     Я бы сказал, что для следующего слоя результат будет означать тоже что dE|dh
    # В виде матричного умножения можно решить как dE|dt * wT
    # Для последнего слоя обычно функция активации не применяется, зато там есть различные методы вычисления и нормализации ошибки.
    # <<< функция активации >>>
    # < для примера используем ReLU >
    # F(t) = max(0,t)
    # F'(t) = 1, t>=0; 0, t<0
    # <<< Пример вычисления ошибки для последнего слоя >>>
    # z = Softmax(t); Этим действием мы нормализуем все значения t в диапозоне 0-1
    # чтобы вычислить софт макс нужно експоненту вычисляемого значения, разделить на сумму експонент всех значений вектора.
    # y - целевое значение.
    # E = CrossEntropy(z,y)
    # Для кроссэнтропии нужно y*ln(z) просумировать результаты и инвертировать знак.
    # Полное упрощение формулы будет следующим, но возможно будет работать только если искомое значение одно и =1.
    # de|dt[i] = S(t[i]) - y[i]; хотя глядя на сокращение кажется что вектор целей независим.
